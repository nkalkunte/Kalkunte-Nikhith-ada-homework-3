---
title: "Kalkunte-Nikhith-HW-3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(cowplot)
library(broom)
library(infer)
```

# Challenge 1
```{r}
f = "https://raw.githubusercontent.com/difiore/ADA-datasets/master/KamilarAndCooperData.csv"
d = read_csv(f)

m = lm(WeaningAge_d~Brain_Size_Species_Mean, data = d)
logM = lm(log(WeaningAge_d)~log(Brain_Size_Species_Mean), data = d)


g  = ggplot(data = d, aes(x = Brain_Size_Species_Mean, y = WeaningAge_d)) + 
  geom_point() +
  geom_abline(slope = m$coefficients[2],intercept = m$coefficients[1])+
  ggtitle("Untransformed Model") + geom_text(x = 350, y = 300, label = paste0("y = ",round(m$coefficients[2],digits = 2),"x + ",round(m$coefficients[1],digits = 2)))

gLOG = ggplot(data = d, aes(x = log(Brain_Size_Species_Mean), y = log(WeaningAge_d))) + 
  geom_point() +
  geom_abline(slope = logM$coefficients[2],intercept = logM$coefficients[1])+
  ggtitle("Log Trasformed Model") + geom_text(x = 4, y = 4, label = paste0("y = ",round(logM$coefficients[2],digits = 2),"x + ",round(logM$coefficients[1],digits = 2)))

plot_grid(g,gLOG)

```

```{r}
alpha = .1
(m.summary = tidy(m, conf.int = TRUE, conf.level = 1-alpha))
(mLog.summary = tidy(logM, conf.int = TRUE, conf.level = 1-alpha))
```

ID B1 point estimate, outcome of hypothesis test, find 90% CI for slope

```{r}
xvals = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean)
ci.m = tibble(x = xvals$Brain_Size_Species_Mean, 
              lower = confint(m, level  = .90)[2,1]*xvals$Brain_Size_Species_Mean + confint(m,level  = .90)[1,1],
              upper = confint(m, level  = .90)[2,2]*xvals$Brain_Size_Species_Mean + confint(m,level  = .90)[1,2])

ci.mLog = tibble(x = log(xvals$Brain_Size_Species_Mean), 
              lower = confint(logM, level  = .90)[2,1]*log(xvals$Brain_Size_Species_Mean) + confint(logM,level  = .90)[1,1],
              upper = confint(logM, level  = .90)[2,2]*log(xvals$Brain_Size_Species_Mean) + confint(logM,level  = .90)[1,2])


yPredict.m = as_tibble(predict(m, newdata = xvals, interval = 'prediction')) 
yPredict.mLog = as_tibble(predict(logM, newdata = log(xvals), interval = 'prediction')) 
yPredict.m$x = xvals$Brain_Size_Species_Mean
yPredict.mLog$x = log(xvals$Brain_Size_Species_Mean)
h.m = g + 
  geom_line(data = ci.m, aes(x = x, y = lower, color = "red")) +
  geom_line(data = ci.m, aes(x = x, y = upper, color = "red")) +
  geom_line(data = yPredict.m, aes(x=x,y=lwr, color = "blue")) + 
  geom_line(data = yPredict.m, aes(x=x,y=upr, color = "blue")) +
  scale_colour_manual(name = '', values =c('red'='red','blue'='blue'), labels = c('90% CI','Prediction Int.'))


h.mLog = gLOG + 
  geom_line(data = ci.mLog, aes(x = x, y = lower, color = "red")) +
  geom_line(data = ci.mLog, aes(x = x, y = upper, color = "red")) +
  geom_line(data = yPredict.mLog, aes(x=x,y=lwr, color = "blue")) + 
  geom_line(data = yPredict.mLog, aes(x=x,y=upr, color = "blue")) +
  scale_colour_manual(name = '', values =c('red'='red','blue'='blue'), labels = c('90% CI','Prediction Int.'))

(predict(m, newdata = data.frame(Brain_Size_Species_Mean = 750), interval = 'prediction'))
(predict(logM, newdata = data.frame(Brain_Size_Species_Mean = 750), interval = 'prediction'))
```
Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
Looking at your two models (i.e., untransformed versus log-log transformed), which do you think is better? Why?

# Challenge 2
```{r}
m2 = lm(data = d, log(MeanGroupSize)~log(Body_mass_female_mean))
m2$coefficients
d$logMeanGroupSize = log(d$MeanGroupSize)
d$logBody_mass_female_mean = log(d$Body_mass_female_mean)
boot.slope =  d %>%
  specify(logMeanGroupSize~logBody_mass_female_mean) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "slope")

boot.intercept = tibble(replicate = boot.slope$replicate, stat = mean(d$logMeanGroupSize,na.rm=TRUE) - mean(d$logBody_mass_female_mean, na.rm=TRUE)*boot.slope$stat)

visualize(boot.slope)
hist(boot.intercept$stat)

alpha <- 0.05
confidence_level <- 1 - alpha
p_lower <- alpha / 2
p_upper <- 1 - (alpha / 2)


boot.slope.summary <- boot.slope %>%
  summarize(
    estimate = mean(stat),
    std.error = sd(stat),
    boot.lower = quantile(stat, p_lower),
    boot.upper = quantile(stat, p_upper)
  )

boot.intercept.summary <- boot.intercept %>%
  summarize(
    estimate = mean(stat),
    std.error = sd(stat),
    boot.lower = quantile(stat, p_lower),
    boot.upper = quantile(stat, p_upper)
  )

```
How do the SEs estimated from the bootstrap sampling distribution compare to those estimated mathematically as part of lm() function?
How do your bootstrap CIs compare to those estimated mathematically as part of the lm() function?

# Challenge 3
```{r}
boot_lm = function(d,model,conf.level = 0.95,reps=1000){
 alpha = 1-conf.level
p_lower <- alpha / 2
p_upper <- 1 - (alpha / 2)

f = as.formula(model)
m = lm(data = d,f)
true = tidy(m,conf.int = TRUE, conf.level = 1 - alpha)[,c(2,3,6,7)]

boot.slope =  d %>%
  specify(f) %>%
  generate(reps = reps, type = "bootstrap") %>%
  calculate(stat = "slope")

b = d %>%
  specify(f) %>%
  generate(reps = 1000, type = "bootstrap")
x = colnames(b)[3]
y = colnames(b)[2]
b.1 = b %>%
  group_by(replicate) %>%
  summarise_at(
    .cols = c(x,y),
    .funs = c(mean = "mean")
  )

boot.intercept = tibble(replicate = boot.slope$replicate, stat = unlist(b.1[,3] - b.1[,2]*boot.slope$stat))

boot.intercept.summary <- boot.intercept %>%
  summarize(
    estimate = mean(stat),
    std.error = sd(stat),
    conf.low = quantile(stat, p_lower),
    conf.high = quantile(stat, p_upper)
  )

boot.slope.summary <- boot.slope %>%
  summarize(
    estimate = mean(stat),
    std.error = sd(stat),
    conf.low = quantile(stat, p_lower),
    conf.high = quantile(stat, p_upper)
  )

data = rbind(c("Original B0",true[2,]),
             c("Original B1",true[1,]),
             c("Bootstrapped B0",boot.slope.summary),
             c("Bootstrapped B1",boot.intercept.summary))
return(data)
}

d$logDayLength_km = log(d$DayLength_km)

(M1 = boot_lm(d,model = "logMeanGroupSize ~ logBody_mass_female_mean"))
(M2 = boot_lm(d,model = "logDayLength_km ~ logBody_mass_female_mean"))
(M3 = boot_lm(d,model = "logDayLength_km ~ logBody_mass_female_mean + logMeanGroupSize"))

```
I wasn't able to get the multiple regressions working in my function as written. 

